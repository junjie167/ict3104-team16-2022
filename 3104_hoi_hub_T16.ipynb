{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf17a53",
   "metadata": {},
   "source": [
    "# <u>T16 HOI HUB</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002b842",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b76b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import json\n",
    "from tqdm.notebook import trange, tqdm ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd853c0",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "046cd00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder = os.listdir('data/video')\n",
    "\n",
    "list_of_video = {}\n",
    "for video in video_folder:\n",
    "    # remove hidden file especially for macOS\n",
    "    if not video.startswith('.'):\n",
    "        list_of_video[video] = video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7753b833",
   "metadata": {},
   "source": [
    "## Create Caption files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d134d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read action and frame object from file\n",
    "def readCaptionFile(filename,videoName):\n",
    "    # Opening JSON file\n",
    "    f = open(filename)\n",
    "\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    data = json.load(f)\n",
    "\n",
    "    # Iterating through the json\n",
    "    # Closing file\n",
    "    f.close()\n",
    "    return data[videoName][\"actions\"]\n",
    "\n",
    "# convert frame to time\n",
    "def convertFrameToTime(frame):\n",
    "    seconds = int(frame/25)\n",
    "    minutes = \"00\"\n",
    "    if seconds >= 60:\n",
    "        minutes = str(seconds // 60)\n",
    "        seconds = seconds % 60\n",
    "    if len(minutes) == 1:\n",
    "        minutes = \"0\" + minutes\n",
    "    seconds = str(seconds)\n",
    "    #may need handle hour\n",
    "    if len(seconds) == 1:\n",
    "        seconds = \"0\" + seconds \n",
    "    return (minutes + \":\" + seconds + \".000\")\n",
    "\n",
    "# read reference text from txt file\n",
    "def readReferenceFile(refFile):\n",
    "    referenceDict = {}\n",
    "    with open(refFile) as f:\n",
    "        lines = f.readlines()\n",
    "    for i in lines:\n",
    "        x = i.split()\n",
    "        referenceDict[str(x[0])] = x[1]\n",
    "    return referenceDict\n",
    "\n",
    "# create caption file\n",
    "def formatCaptionFile(captionList, reference, captionPath):\n",
    "    start = \"WEBVTT\\n\\n\"\n",
    "    captions = []\n",
    "    for i in captionList:\n",
    "        text = reference[str(i[0])]\n",
    "        lines = convertFrameToTime(i[1]) + \" --> \" + convertFrameToTime(i[2]) + \"\\n\" + text + \"\\n\\n\"\n",
    "        captions.append(lines)\n",
    "    f = open(captionPath, \"w\")\n",
    "    f.write(start)\n",
    "    f.writelines(captions)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a51743c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d047487733444a7a8cf8159c22404dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Videos', options={'P02T01C07.vtt': 'P02T01C07.vtt', 'P02T01C06.vtt': 'P02T01C06.vtt', 'P…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video_dropdown = widgets.Dropdown(\n",
    "    options = list_of_video,\n",
    "    description = 'Videos',\n",
    ")\n",
    "\n",
    "video_src = 'data/video/' + video_dropdown.value\n",
    "\n",
    "def play_video(video_src,caption_src):\n",
    "    video = io.open(video_src, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    return(HTML(data='''<video width=\"650\" height=\"360\" controls>\n",
    "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "        <track kind=\"captions\" src={1} srclang=\"en\" label=\"English\" default>\n",
    "        </video>'''.format(encoded.decode('ascii'),caption_src)))\n",
    "\n",
    "\n",
    "# video dropdown onchange function\n",
    "def video_on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        change['new']\n",
    "        global video_src \n",
    "        video_src = 'data/video/'+ video_dropdown.value\n",
    "        \n",
    "          \n",
    "# display video dropdown\n",
    "video_dropdown.observe(video_on_change)\n",
    "display(video_dropdown) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa9208bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption\n",
    "videoName = video_dropdown.value.split(\".\")\n",
    "#location of reference are place at root\n",
    "ref = readReferenceFile('all_labels.txt')\n",
    "# may need change the caption path to dynamic\n",
    "captionPath = \"data/video/\" + videoName[0] + \".vtt\"\n",
    "# model result file should be some directory, here using root \n",
    "captionList = readCaptionFile('smarthome_CS_51.json',videoName[0])\n",
    "formatCaptionFile(captionList,ref,captionPath)\n",
    "\n",
    "\n",
    "video = video_src.split('/')[-1]\n",
    "print(\"Currently playing : \" + video)\n",
    "play_video(video_src, captionPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f17fab",
   "metadata": {},
   "source": [
    "## Extract i3D features from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "831d5daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ebc8557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2247ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after the desired videos are selected\n",
    "video_paths: list[str] = [\"../data/RGB_Video_MP4/P02T01C06.mp4\"]\n",
    "# TODO: choose a project-relative directory\n",
    "output_path = \"/media/starlight/2c72c05a-ec96-4c96-ba3c-50ae4bc6730b/home/starlight/TSU/data/RGB_i3d_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fae5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "i3d_defaults = OmegaConf.load(Path(\"feature_extractor/configs/i3d.yml\"))\n",
    "i3d_config = OmegaConf.merge(i3d_defaults, OmegaConf.create({\n",
    "    \"feature_type\": \"i3d\",\n",
    "    \"streams\": \"rgb\",\n",
    "    \"output_path\": output_path,\n",
    "    \"video_paths\": video_paths,\n",
    "    \"on_extraction\": \"save_numpy\",\n",
    "    \"stack_size\": 16,\n",
    "    \"step_size\": 16\n",
    "}))\n",
    "extractor = models.ExtractI3D(i3d_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4dd2182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a77e1edb3004987bb986d1724b3abf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for video in tqdm(video_paths, desc=\"videos extracted\"):\n",
    "    extractor._extract(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a56ee9a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e0a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b06ccb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_SEED!!!: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Example: to be modified\n",
    "# lets say TSU has been selected\n",
    "from TSU_PDAN import HOI_PDAN\n",
    "from HOI.smarthome_i3d_per_video import TSU as Dataset\n",
    "from HOI.smarthome_i3d_per_video import TSU_collate_fn as collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bad61a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 536/536 [00:05<00:00, 95.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO: v-iashin\n",
    "# lets say TSU smarthome is then selected (in practice, a custom json is generated depending on what specific videos are selected)\n",
    "val_dataset = Dataset(\"HOI/data/smarthome_CS_51.json\", 'testing', \"../data/RGB_i3d_16frames_64000_SSD\", 1, 51)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=4,\n",
    "                                                pin_memory=True, collate_fn=collate_fn)\n",
    "# val_dataloader.root = args.rgb_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b28d14c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/starlight/2c72c05a-ec96-4c96-ba3c-50ae4bc6730b/home/starlight/TSU/ict3104-team16-2022/HOI/models.py:88: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.key_conv.weight, mode='fan_out')\n",
      "/media/starlight/2c72c05a-ec96-4c96-ba3c-50ae4bc6730b/home/starlight/TSU/ict3104-team16-2022/HOI/models.py:89: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.value_conv.weight, mode='fan_out')\n",
      "/media/starlight/2c72c05a-ec96-4c96-ba3c-50ae4bc6730b/home/starlight/TSU/ict3104-team16-2022/HOI/models.py:90: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.query_conv.weight, mode='fan_out')\n",
      "/media/starlight/2c72c05a-ec96-4c96-ba3c-50ae4bc6730b/home/starlight/TSU/ict3104-team16-2022/HOI/models.py:91: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(self.rel_t, 0, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelrunner = HOI_PDAN()\n",
    "modelrunner.PDAN_training_parameters()\n",
    "modelrunner.model.load_state_dict(torch.load(\"HOI/PDAN/weight_epoch_0.0002_35\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a5a89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8a12f7bf4d446184bda20042c2726a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/185 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/starlight/2c72c05a-ec96-4c96-ba3c-50ae4bc6730b/home/starlight/TSU/ict3104-team16-2022/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/media/starlight/2c72c05a-ec96-4c96-ba3c-50ae4bc6730b/home/starlight/TSU/ict3104-team16-2022/.venv/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/media/starlight/2c72c05a-ec96-4c96-ba3c-50ae4bc6730b/home/starlight/TSU/ict3104-team16-2022/apmeter.py:108: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  rg = torch.range(1, self.scores.size(0)).float()\n",
      "/media/starlight/2c72c05a-ec96-4c96-ba3c-50ae4bc6730b/home/starlight/TSU/ict3104-team16-2022/apmeter.py:137: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  ap[k] = precision[truth.byte()].sum() / max(truth.sum(), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val-map: tensor(34.1454)\n",
      "tensor([46.0981, 53.4290, 44.6538, 51.1127, 43.1801, 46.4834, 37.1899, 31.5516,\n",
      "        42.9660, 21.3682, 17.6236, 13.5470,  1.1666, 41.1923, 49.6549,  3.8188,\n",
      "        19.1954, 59.6317,  5.5071, 71.6044, 36.5219, 74.4247, 54.4266,  3.0952,\n",
      "         0.8215, 62.6654, 35.2815, 45.3404, 59.1798, 16.6004,  7.4304,  0.3018,\n",
      "        33.3252,  3.7282, 43.4654, 88.7000, 37.3753, 18.4837, 25.2225,  5.6187,\n",
      "        71.5390, 33.2785, 13.1370, 62.3055,  3.1802, 80.0095, 72.6115, 29.4839,\n",
      "        18.2610,  3.3478,  1.2794])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(34.1454)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = None\n",
    "# note: this doesn't appear to show up properly in vscode\n",
    "with tqdm(val_dataloader, unit='batch') as progressive_loader:\n",
    "    result = modelrunner.evaluate(progressive_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6e5012",
   "metadata": {},
   "source": [
    "## Training HOI ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d461e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1751661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 536/536 [00:26<00:00, 19.90it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(\"HOI/data/smarthome_CS_51.json\", 'training', \"../data/RGB_i3d_16frames_64000_SSD\", 1, 51)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4,\n",
    "                                                pin_memory=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8447ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670aebbc9db347fab0bd649c8aa849d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d8de550dfc46be9fdf5258d7a3f278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/351 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a1ca804e4345d1a50bad1c3377971c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validating:   0%|          | 0/185 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-map: tensor(7.4005)\n",
      "val-map: tensor(13.6736)\n",
      "tensor([1.4556e+01, 4.1833e+01, 8.9046e+00, 5.9780e+00, 9.0657e-01, 1.7288e+01,\n",
      "        1.4202e+00, 5.1075e+00, 1.1310e+01, 4.6842e+00, 4.6598e+00, 1.3473e+00,\n",
      "        2.7479e-01, 4.9669e+00, 2.2991e+01, 3.9146e+00, 1.4208e+00, 3.0398e+01,\n",
      "        1.7539e+00, 5.5832e+01, 5.0160e+00, 7.3792e+00, 2.8778e+01, 5.3361e+00,\n",
      "        1.9476e-01, 3.9621e+01, 2.7845e+01, 1.1070e+01, 3.4978e+01, 1.1449e+00,\n",
      "        9.7553e-01, 9.0512e-03, 3.6942e+00, 6.9876e-01, 2.2886e+01, 4.5529e+01,\n",
      "        7.0930e+00, 2.9289e+00, 9.9866e+00, 1.1697e+00, 2.5055e+01, 4.5129e+00,\n",
      "        2.0219e+01, 5.1237e+01, 5.4919e-01, 3.3501e+01, 2.3606e+01, 2.6887e+01,\n",
      "        1.5436e+01, 3.7692e-01, 9.5107e-02])\n",
      "train-map: tensor(10.1407)\n",
      "val-map: tensor(16.0195)\n",
      "tensor([1.4440e+01, 4.5139e+01, 1.0591e+01, 1.4382e+01, 1.1897e+00, 1.9334e+01,\n",
      "        1.2840e+00, 7.4700e+00, 1.1914e+01, 5.6119e+00, 5.1394e+00, 2.2644e+00,\n",
      "        5.6212e-01, 5.9308e+00, 2.2420e+01, 4.5152e+00, 4.3036e+00, 3.7324e+01,\n",
      "        2.8105e+00, 6.1163e+01, 7.3787e+00, 1.4015e+01, 3.1940e+01, 5.6064e+00,\n",
      "        2.0461e-01, 4.9340e+01, 2.6393e+01, 1.2542e+01, 4.4596e+01, 1.5883e+00,\n",
      "        9.6337e-01, 8.1042e-03, 3.3552e+00, 6.4548e-01, 3.0411e+01, 6.5150e+01,\n",
      "        6.6546e+00, 4.8157e+00, 1.2176e+01, 1.9586e+00, 2.6069e+01, 4.7096e+00,\n",
      "        2.2538e+01, 5.3092e+01, 7.9966e-01, 4.0903e+01, 2.9514e+01, 2.9305e+01,\n",
      "        1.1919e+01, 4.9863e-01, 1.1391e-01])\n",
      "train-map: tensor(13.1511)\n",
      "val-map: tensor(17.0431)\n",
      "tensor([1.9379e+01, 4.8132e+01, 1.1938e+01, 1.5689e+01, 1.3339e+00, 1.8641e+01,\n",
      "        1.9698e+00, 5.0426e+00, 1.5102e+01, 5.8979e+00, 5.8013e+00, 2.4991e+00,\n",
      "        6.5932e-01, 6.4444e+00, 2.6075e+01, 3.1720e+00, 4.1508e+00, 4.0673e+01,\n",
      "        1.6736e+00, 5.6790e+01, 9.6214e+00, 1.7972e+01, 3.3938e+01, 7.1129e+00,\n",
      "        2.4268e-01, 5.2305e+01, 2.9050e+01, 1.6566e+01, 4.9259e+01, 2.3351e+00,\n",
      "        2.6659e+00, 8.7304e-03, 6.3397e+00, 1.1221e+00, 3.0918e+01, 6.6271e+01,\n",
      "        7.9788e+00, 4.3745e+00, 1.3587e+01, 2.1901e+00, 2.9964e+01, 4.9650e+00,\n",
      "        2.2010e+01, 5.5442e+01, 6.4736e-01, 3.4934e+01, 3.0343e+01, 2.9383e+01,\n",
      "        1.5796e+01, 6.3444e-01, 1.5712e-01])\n",
      "train-map: tensor(14.4803)\n",
      "val-map: tensor(17.1550)\n",
      "tensor([1.9978e+01, 5.0385e+01, 1.1480e+01, 1.6274e+01, 2.3113e+00, 2.8719e+01,\n",
      "        4.0613e+00, 8.5228e+00, 1.5757e+01, 5.9263e+00, 6.2348e+00, 1.1197e+00,\n",
      "        2.2778e-01, 5.3184e+00, 2.6902e+01, 3.5461e+00, 3.5864e+00, 4.0782e+01,\n",
      "        1.8119e+00, 6.1169e+01, 9.7081e+00, 1.7467e+01, 3.4043e+01, 4.6287e+00,\n",
      "        2.1605e-01, 5.0361e+01, 2.5896e+01, 1.5525e+01, 5.0178e+01, 1.5574e+00,\n",
      "        1.1630e+00, 8.7846e-03, 4.3687e+00, 6.5297e-01, 2.3362e+01, 6.9033e+01,\n",
      "        5.5936e+00, 4.2027e+00, 8.2235e+00, 3.0046e+00, 3.5014e+01, 7.4569e+00,\n",
      "        2.6795e+01, 5.0507e+01, 8.4988e-01, 3.6123e+01, 3.5350e+01, 2.6730e+01,\n",
      "        1.2174e+01, 4.5540e-01, 1.4412e-01])\n",
      "train-map: tensor(16.3483)\n",
      "val-map: tensor(18.2657)\n",
      "tensor([2.0278e+01, 5.0284e+01, 1.1939e+01, 1.4862e+01, 1.9393e+00, 3.1139e+01,\n",
      "        3.9342e+00, 7.4326e+00, 1.5667e+01, 4.9975e+00, 5.4751e+00, 1.2193e+00,\n",
      "        2.6640e-01, 5.1643e+00, 2.8093e+01, 3.1932e+00, 3.7543e+00, 4.0259e+01,\n",
      "        2.6105e+00, 6.2202e+01, 1.1668e+01, 2.1273e+01, 3.5686e+01, 6.0995e+00,\n",
      "        2.5583e-01, 5.3784e+01, 2.6906e+01, 1.4521e+01, 5.4707e+01, 1.9596e+00,\n",
      "        1.5676e+00, 8.7374e-03, 4.6266e+00, 5.6290e-01, 2.9685e+01, 7.3702e+01,\n",
      "        7.4808e+00, 6.3232e+00, 1.1126e+01, 3.3812e+00, 3.0281e+01, 8.2581e+00,\n",
      "        2.7773e+01, 6.1736e+01, 7.4879e-01, 4.4732e+01, 3.6739e+01, 2.6873e+01,\n",
      "        1.3856e+01, 4.2518e-01, 9.5106e-02])\n"
     ]
    }
   ],
   "source": [
    "with trange(0,5, unit='epoch', desc='epochs') as epoch_range:\n",
    "    with tqdm(train_dataloader, unit='batch', desc='training') as train_loader:\n",
    "        with tqdm(val_dataloader, unit='batch', desc='validating') as val_loader:\n",
    "            for model in modelrunner.train(\n",
    "                train_dataloader=train_loader, \n",
    "                val_dataloader=val_loader, \n",
    "                epoch_range=epoch_range\n",
    "            ):\n",
    "                # reset progress bars for next iteration\n",
    "                train_loader.reset()\n",
    "                val_loader.reset()\n",
    "                # save model snapshot at this epoch\n",
    "                torch.save(model.state_dict(),'./data/pretrained_model/PDAN/weight_epoch_'+str(modelrunner.start_epoch))\n",
    "                torch.save(model,'./data/pretrained_model/PDAN/model_epoch_'+str(modelrunner.start_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4bbbf3",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc661c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pipeline_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "eae6df607e7c73293184c4e59a9082282e98ddc34ad3a11fe810a321ff3b5f72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
